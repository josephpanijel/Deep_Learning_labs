{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNx-DlWhUojN"
   },
   "source": [
    "# Deep Learning Course: Lab Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfmNFn5pUojN"
   },
   "source": [
    "In this lab exercise you will become familiar with the PyTorch library in order to solve deep learning problems. The goals of this assignments are as follows:\n",
    "\n",
    "- familiarize with PyTorch Tensors\n",
    "\n",
    "- understand how feedforward backpropagation works in neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKVnjTPMvC_S"
   },
   "source": [
    "First time using a Jupyter Notebook or Google Colab? Check [this Jupyter Notebook 101](https://www.kaggle.com/code/jhoward/jupyter-notebook-101).\n",
    "During all the courses you will be asked more than just applying the lectures: check the documentation, ask what you want to do on your favorite search engine or ask the TAs. The Deep Learning community is really open to new practionners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4j7Gf5LUojO"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efuanY_UUojO"
   },
   "source": [
    "For this exercise the only thing you need is this notebook.\n",
    "\n",
    "You may use your own Python environment or use Google Colab. If you choose to use Google Colab, you can upload this notebook to your Google Drive and open it with Google Colab (right click on the file and choose \"Open with\" -> \"Google Colab\").\n",
    "\n",
    "To set up the environment on your own machine, you need to install PyTorch. You can find the instructions [here](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "For more information about Python environment, you may take a look at [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) or [virtualenv](https://virtualenv.pypa.io/en/latest/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZumXYTYUojO"
   },
   "source": [
    "# Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-6WBdq3UojO"
   },
   "source": [
    "Apart from the Questions, there are instruction comments throughout the notebook as well as comments inside the code cells beginning with two hashtags (##). In addition, there are #**START CODE  /  #**END CODE comments indicating the start and end of your code sections. Pay attention not to delete these comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YQ2FNeqUojP"
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2LYwZGKUojP"
   },
   "source": [
    "# Q1 - PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOus3aXGUojP"
   },
   "source": [
    "a) Get familiar with PyTorch Tensors and construct different types of them. You may take a look at the [documentation](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g95S8R61UojP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "##Construct a 5x3 matrix, uninitialized\n",
    "# *****START CODE\n",
    "#x = torch.empty((5, 3))\n",
    "x = torch.ones((5, 3))\n",
    "# *****END CODE\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MkCzwPzxUojQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5410, -0.2934, -2.1788],\n",
      "        [ 0.5684, -1.0845, -1.3986],\n",
      "        [ 0.4033,  0.8380, -0.7193],\n",
      "        [-0.4033, -0.5966,  0.1820],\n",
      "        [-0.8567,  1.1006, -1.0712]])\n"
     ]
    }
   ],
   "source": [
    "##Construct a randomly initialized matrix from a normal distribution\n",
    "# *****START CODE\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn((5, 3))\n",
    "# *****END CODE\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pe99pXJHUojQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "##Construct a matrix filled with zeros and of dtype int64\n",
    "# *****START CODE\n",
    "x = torch.zeros((5, 3), dtype=torch.int64)\n",
    "# *****END CODE\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R43-VxQ6UojQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "##Construct a tensor directly from data\n",
    "# *****START CODE\n",
    "data = [[1, 2], [3, 4]]\n",
    "x = torch.tensor(data)\n",
    "# *****END CODE\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UORgptIYUojR"
   },
   "source": [
    "#Q2 Backpropagation from scratch\n",
    "\n",
    "- Create random input and output PyTorch tensors and train a simple network from scratch.\n",
    "\n",
    "  Warning: You should NOT use any forward/backward commands from PyTorch             library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QtrAESF-UojR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([64, 1000])\n",
      "y torch.Size([64, 10])\n",
      "tensor([[ 0.4395, -0.7543, -1.1628,  2.0784, -0.1075, -0.6689,  2.8065,  0.2962,\n",
      "         -1.5878, -0.8216],\n",
      "        [-1.2372,  1.1736, -0.0251,  1.2981, -0.4053,  0.6879,  0.5738,  1.1403,\n",
      "          1.1085, -0.2007],\n",
      "        [-0.0950,  0.5540, -0.2009,  0.5140, -0.9023,  0.4620,  1.1448,  0.5863,\n",
      "          0.0288,  0.9081],\n",
      "        [ 1.3919, -0.6782, -0.5078,  1.3325, -1.4151, -0.9260, -2.0968, -1.4919,\n",
      "          0.5533, -0.5315],\n",
      "        [-0.4725,  0.5125,  1.3638, -0.4927, -0.1316,  0.6849,  0.8089,  0.1475,\n",
      "         -1.3047, -1.2748],\n",
      "        [ 0.0359, -0.6505,  1.1541,  1.7395,  1.7768,  0.7478,  0.1213,  0.7259,\n",
      "          2.0841,  0.3146],\n",
      "        [-0.2737, -0.3709,  0.7325, -0.2763,  1.8841,  1.4673,  1.3834, -0.3437,\n",
      "          1.0297, -0.0540],\n",
      "        [-0.6766,  0.6351,  0.0474, -1.3022, -0.1161,  1.1456, -1.6812, -0.7351,\n",
      "          0.5064,  0.4201],\n",
      "        [ 0.8409, -1.6253, -1.3315,  1.0351,  0.5222, -0.2645, -1.0191, -1.0789,\n",
      "          0.2307,  0.0575],\n",
      "        [ 0.0574,  1.0236,  0.0976, -0.7694, -1.1892,  0.4839, -0.6310, -0.6885,\n",
      "          0.6616, -0.4122],\n",
      "        [-0.6279,  0.5098, -0.2787,  1.0049, -0.7926, -0.1954, -1.6959,  0.8766,\n",
      "          0.9289,  1.1828],\n",
      "        [ 0.5975, -0.5571,  0.3104,  1.6769, -1.2715,  0.4491, -0.4104,  0.0950,\n",
      "          0.5466, -1.6113],\n",
      "        [-1.0022, -0.5360, -1.7622, -0.3808, -0.2402,  1.2436,  2.7139,  0.7152,\n",
      "         -0.5257,  0.6873],\n",
      "        [ 1.1372, -0.0797, -0.4402, -0.9647,  0.6419,  1.0561,  0.2174,  1.8606,\n",
      "          0.5651,  0.3842],\n",
      "        [ 0.6804, -1.0771, -0.5399, -0.3777, -1.5460, -1.3472,  1.3726, -0.5949,\n",
      "         -1.4753,  0.1414],\n",
      "        [-0.7818,  0.0660,  0.1472, -0.7305,  1.2030, -0.7789, -0.2563, -0.4808,\n",
      "          0.2806, -0.9185],\n",
      "        [ 0.8218,  1.6087, -0.3189,  0.8975, -0.0571,  0.3921,  1.1385, -0.0252,\n",
      "          0.5837,  1.6608],\n",
      "        [ 0.8732,  1.9630, -0.3365,  1.0870, -0.1353,  0.8270, -0.7945, -0.9769,\n",
      "          0.5394, -0.8916],\n",
      "        [-2.2298, -0.4946,  0.2756,  0.3448,  0.9984,  1.0115,  0.7830,  0.7826,\n",
      "          0.7262, -0.7820],\n",
      "        [ 0.7852, -0.0379,  0.9131, -0.3460,  0.4246,  1.5592,  0.6033,  0.0819,\n",
      "         -0.1938,  0.6482],\n",
      "        [-0.9817, -1.2884, -0.6539,  0.7387,  0.7938,  1.2576,  0.5468, -0.2395,\n",
      "         -0.6511,  0.5309],\n",
      "        [ 0.3591,  0.8521,  0.0501,  0.1358, -0.2813, -0.5113, -0.0825,  1.0437,\n",
      "         -0.2962, -0.7831],\n",
      "        [-0.2650,  0.4129,  2.5922, -1.7641,  0.0132, -0.4916, -1.0417, -1.0696,\n",
      "          0.1446, -1.2224],\n",
      "        [ 0.4022, -0.0649,  1.2461, -0.2584,  0.5937, -0.5659, -0.4558,  0.7506,\n",
      "          2.4674,  0.7576],\n",
      "        [-0.3556, -1.5995,  1.3428, -0.1734,  0.0284,  0.4505, -0.2539, -0.5438,\n",
      "          0.7035,  1.1226],\n",
      "        [ 0.6530,  1.5505,  1.0515,  1.5106, -1.1839,  0.0343,  0.6324,  0.2141,\n",
      "          0.5075,  1.6221],\n",
      "        [-1.8580, -1.6032, -0.4268,  0.9736, -1.0453,  0.3059,  0.7384,  1.1779,\n",
      "          0.4784,  0.8141],\n",
      "        [-0.5826, -1.4466,  1.0082, -2.3929,  0.0470,  1.4440,  0.9015,  2.0903,\n",
      "          0.3943, -0.3424],\n",
      "        [-0.1260, -0.4035,  1.7361, -0.1842, -0.6310,  0.6645, -1.1495, -2.9276,\n",
      "          0.3838, -1.0051],\n",
      "        [-1.8695, -0.1507,  0.7098, -0.2949,  0.2293,  0.5444, -1.3355, -0.8144,\n",
      "          0.4200,  1.0925],\n",
      "        [ 0.2075, -0.1453,  0.0692,  0.4075,  1.2937,  1.0806,  0.5500,  0.0890,\n",
      "          0.0069, -0.3398],\n",
      "        [-0.6085, -1.7385,  0.0123,  0.4600, -2.3716,  0.7592,  1.3949,  0.2302,\n",
      "         -1.0426, -0.1356],\n",
      "        [-0.4889,  1.1497, -1.1832,  1.0754,  1.1814, -0.3284,  0.2340,  0.2538,\n",
      "          0.6335,  2.2070],\n",
      "        [ 0.3210,  0.1577,  0.9012,  0.2089, -0.3208, -0.3616, -0.8749,  1.0817,\n",
      "          1.0342, -1.5855],\n",
      "        [ 0.4649,  0.6154,  0.3125, -0.5880, -0.1206, -1.6268, -0.1536,  0.6325,\n",
      "         -0.7978,  0.1977],\n",
      "        [ 0.5790,  1.5570,  1.1946, -0.7283,  0.4725, -0.6195,  1.0450, -0.6969,\n",
      "          0.6320,  0.3927],\n",
      "        [-0.2608, -2.0859,  1.5774,  2.1088,  0.7598, -0.0609,  1.4019, -1.3302,\n",
      "         -0.3508,  1.7854],\n",
      "        [-0.9356, -0.3364,  1.4133,  2.5493, -0.4382,  0.3888,  0.4204, -0.9701,\n",
      "          0.1577,  0.6724],\n",
      "        [ 0.6893,  1.2579,  0.2083,  1.3042, -1.0168,  0.0340,  0.0469, -0.6558,\n",
      "         -0.0518,  0.5340],\n",
      "        [-0.3678, -0.1498, -1.2439, -0.3570, -0.3431,  0.1477, -0.7672, -1.1869,\n",
      "         -0.1114,  1.4483],\n",
      "        [-0.6161,  0.9112,  0.7630, -2.0852, -0.1191, -0.1555, -0.8477,  1.0705,\n",
      "          0.4962,  0.6150],\n",
      "        [-0.5229, -1.0617,  1.0935, -2.4139, -0.7018, -0.0162, -0.1001, -0.5742,\n",
      "          0.1759,  0.3860],\n",
      "        [-1.1068, -0.7995,  1.0835, -0.2452, -1.8800,  0.5176, -0.3575,  1.3691,\n",
      "         -1.3932, -0.7114],\n",
      "        [-0.0525, -0.1221,  1.5238,  2.1846,  2.5506, -0.6829,  1.3669,  0.0119,\n",
      "          0.4291, -0.3623],\n",
      "        [ 0.8987, -0.2307, -0.3734,  0.8458, -0.3280, -0.1431,  1.3048,  0.1004,\n",
      "          0.0423, -0.3243],\n",
      "        [-1.4282,  0.5009, -0.1453,  0.7153, -0.5958,  0.6991,  0.8315,  0.6383,\n",
      "          0.4576,  0.3802],\n",
      "        [-1.0088,  0.4358, -1.2970,  0.3390,  1.1974, -0.6632, -1.2585,  0.2092,\n",
      "          0.1964,  1.1794],\n",
      "        [-0.2479, -0.8427, -0.7974,  0.5994, -0.2698, -1.8106,  1.9665,  1.0475,\n",
      "          0.0210,  0.0741],\n",
      "        [-0.4212, -1.2169,  0.4855,  1.8696,  1.4906,  0.6251, -1.5175, -1.5052,\n",
      "         -1.2002, -2.0929],\n",
      "        [-1.4491,  0.2790, -0.0419,  0.7560,  0.0848, -0.8891, -0.5680, -1.0315,\n",
      "         -0.4335,  0.0978],\n",
      "        [ 0.3015,  2.2311, -0.5179, -0.9386, -0.3468,  0.9422, -1.8049, -0.0125,\n",
      "          0.4795,  1.4230],\n",
      "        [ 0.5738,  1.5551,  0.6676, -0.7618, -1.3088,  0.0303,  0.7503, -0.8915,\n",
      "          1.3388, -0.1440],\n",
      "        [ 2.1481,  1.2850, -0.8668, -0.1087,  0.9905, -2.4513,  1.7753,  0.0831,\n",
      "         -1.9357, -0.8786],\n",
      "        [ 0.5997,  0.5386,  1.3138,  1.7331,  0.2818,  1.5347, -1.4026, -0.7066,\n",
      "          0.3571, -0.8985],\n",
      "        [-0.4169,  0.0434,  1.0509,  0.9801, -0.0533, -0.4207, -1.4667,  0.2748,\n",
      "         -0.7182, -0.1825],\n",
      "        [ 0.8006, -0.0313,  0.8961,  0.0422,  0.0273,  1.3926, -2.1469, -1.1808,\n",
      "         -1.4897, -0.4691],\n",
      "        [ 0.6483,  0.4672, -0.1385, -0.9521, -0.4391,  1.1851,  0.8422, -1.6778,\n",
      "         -1.0226, -2.2173],\n",
      "        [ 1.0359,  0.6844,  1.1207,  1.9020, -0.5978, -1.4747, -0.5625, -0.6775,\n",
      "          0.2607,  1.3668],\n",
      "        [-0.4138,  0.3691, -0.5313,  0.9543, -0.7989,  1.7960, -0.4839, -0.1598,\n",
      "         -0.2136, -1.1065],\n",
      "        [-0.8205,  0.0632, -0.1349, -0.1707, -0.3026,  0.5953, -1.9363, -0.6384,\n",
      "         -0.6848, -1.1061],\n",
      "        [ 1.7718,  0.0683,  1.0207,  0.8452,  0.6433, -0.1222,  1.1167, -0.2351,\n",
      "          0.9994,  0.0258],\n",
      "        [ 0.0749,  2.2844, -0.0928,  0.8888, -0.7096,  0.3449, -1.1117, -0.6884,\n",
      "          1.1265,  0.0861],\n",
      "        [ 0.1648,  0.2119,  0.5892,  0.3259, -0.9806, -0.4610, -0.8188,  1.8507,\n",
      "          0.4049, -0.1588],\n",
      "        [ 0.0531,  0.0580, -0.1621,  0.2045,  2.1860,  0.8586, -0.6676, -0.4030,\n",
      "         -0.1112, -0.1268]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "## N is batch size; D_in is input dimension\n",
    "## H is hidden dimenion; D_out is output dimension\n",
    "\n",
    "torch.manual_seed(0)\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "## Create random input (x) and output (y) data\n",
    "# *****START CODE\n",
    "x = torch.randn((N, D_in))\n",
    "y = torch.randn((N, D_out))\n",
    "print('x', x.shape)\n",
    "print('y', y.shape)\n",
    "# *****END CODE\n",
    "print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zofILGJ3VCBz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 torch.Size([1000, 100])\n",
      "w2 torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "##Randomly initialize weights from a normal distribution, skipping bias\n",
    "##Hint: You need 2 weight tensors; one for the raw input tensor (x) and one for the hidden dimension\n",
    "# *****START CODE\n",
    "w1 = torch.randn((D_in, H))\n",
    "w2 = torch.randn((H, D_out))\n",
    "# *****END CODE\n",
    "print('w1', w1.shape)\n",
    "print('w2', w2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9sQA5kvXVLge"
   },
   "outputs": [],
   "source": [
    "##define the learning rate\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xgDIJGcVfnb"
   },
   "source": [
    "First, implement the forward pass. Try to compute the predicted y_pred value. You can take a look [here](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activations-weighted-sum-nonlinearity) for more information about activation functions in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OlQ0tMGsVQrz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([64, 1000])\n",
      "h torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "## Calculate the output of the hidden dimension\n",
    "## Hint: make use of torch.matmul()\n",
    "# *****START CODE\n",
    "import torch.nn.functional as F\n",
    "# x : N x D_in\n",
    "# w1 : D_in x H\n",
    "# h : N x H\n",
    "#h = torch.matmul(x, w1)\n",
    "h = x @ w1                # output of the hidden dimension \n",
    "# *****END CODE\n",
    "print('x', x.shape)\n",
    "print('h', h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8NbtF0QtWA25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "## Pass the output of the hidden dimension to the ReLU activation function\n",
    "# *****START CODE\n",
    "h_relu = F.relu(h)           # output of the ReLU function\n",
    "# *****END CODE  \n",
    "print(h_relu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "xAMaSc7JWA-E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred torch.Size([64, 10])\n",
      "y torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "## Calculate the final output of the network\n",
    "# *****START CODE\n",
    "# h_relu : N x H\n",
    "# w2 : H x D_out\n",
    "# y_pred : N x D_out\n",
    "y_pred = h_relu @ w2           # final output of the network\n",
    "# *****END CODE\n",
    "print('y_pred', y_pred.shape)\n",
    "print('y', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xHdnKovWvIk"
   },
   "source": [
    "Calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "usvfBgnWVQ4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45982.2891)\n"
     ]
    }
   ],
   "source": [
    "## Compute loss\n",
    "loss = ((y_pred - y) ** 2).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QklmgLMnW1k_"
   },
   "source": [
    "Now, implement the backward pass.\n",
    "You need to minimize the loss with respect to each weight using the chain rule of differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "rOlsl-15W35z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss_d_y_pred torch.Size([64, 10])\n",
      "d_y_pred_d_w2 torch.Size([64, 100])\n",
      "w2 torch.Size([100, 10])\n",
      "d_loss_d_w2 torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "## Compute the gradient of w2 with respect to the loss\n",
    "# *****START CODE\n",
    "d_loss_d_y_pred = 2.0 * (y_pred - y)\n",
    "d_y_pred_d_w2 = h_relu\n",
    "\n",
    "print('d_loss_d_y_pred', d_loss_d_y_pred.shape)\n",
    "print('d_y_pred_d_w2', d_y_pred_d_w2.shape)\n",
    "\n",
    "d_loss_d_w2 = d_y_pred_d_w2.T @ d_loss_d_y_pred\n",
    "\n",
    "print('w2', w2.shape)\n",
    "print('d_loss_d_w2', d_loss_d_w2.shape)\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "PVWLJdcwXF_C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss_d_y_pred torch.Size([64, 10])\n",
      "d_y_pred_d_h torch.Size([100, 10])\n",
      "d_h_d_w1 torch.Size([64, 1000])\n",
      "d_loss_d_w1 first step torch.Size([64, 100])\n",
      "d_loss_d_w1 torch.Size([1000, 100])\n",
      "w1 torch.Size([1000, 100])\n"
     ]
    }
   ],
   "source": [
    "## Compute the gradient of w1 with respect to the loss (consider the derivative of ReLU equal to 1)\n",
    "# *****START CODE\n",
    "d_loss_d_y_pred = 2.0 * (y_pred - y)\n",
    "d_y_pred_d_h = w2\n",
    "d_h_d_w1 = x\n",
    "\n",
    "print('d_loss_d_y_pred', d_loss_d_y_pred.shape)\n",
    "print('d_y_pred_d_h', d_y_pred_d_h.shape)\n",
    "print('d_h_d_w1', d_h_d_w1.shape)\n",
    "\n",
    "d_loss_d_w1 = d_loss_d_y_pred @ d_y_pred_d_h.t()\n",
    "print('d_loss_d_w1 first step', d_loss_d_w1.shape)\n",
    "d_loss_d_w1 = d_h_d_w1.t() @ d_loss_d_w1\n",
    "print('d_loss_d_w1', d_loss_d_w1.shape)\n",
    "print('w1', w1.shape)\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "cdZx29tlXNUx"
   },
   "outputs": [],
   "source": [
    "## Update weights\n",
    "# *****START CODE\n",
    "w1 = w1 - learning_rate * d_loss_d_w1\n",
    "w2 = w2 - learning_rate * d_loss_d_w2\n",
    "# *****END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DWDA8CZXU5p"
   },
   "source": [
    "Repeat the above process for a number of epochs and notice how the value of the loss changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "jWbfq-36XkRN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 0.5558471083641052\n",
      "Epoch 1 loss 0.5234987735748291\n",
      "Epoch 2 loss 0.4931202828884125\n",
      "Epoch 3 loss 0.46459174156188965\n",
      "Epoch 4 loss 0.4377848207950592\n",
      "Epoch 5 loss 0.41260281205177307\n",
      "Epoch 6 loss 0.3889264464378357\n",
      "Epoch 7 loss 0.36666855216026306\n",
      "Epoch 8 loss 0.34573671221733093\n",
      "Epoch 9 loss 0.3260573148727417\n",
      "Epoch 10 loss 0.3075433373451233\n",
      "Epoch 11 loss 0.2901175618171692\n",
      "Epoch 12 loss 0.27373018860816956\n",
      "Epoch 13 loss 0.2582905888557434\n",
      "Epoch 14 loss 0.24375469982624054\n",
      "Epoch 15 loss 0.23007865250110626\n",
      "Epoch 16 loss 0.21718856692314148\n",
      "Epoch 17 loss 0.20505526661872864\n",
      "Epoch 18 loss 0.19361460208892822\n",
      "Epoch 19 loss 0.18283094465732574\n",
      "Epoch 20 loss 0.17267245054244995\n",
      "Epoch 21 loss 0.16309267282485962\n",
      "Epoch 22 loss 0.1540582925081253\n",
      "Epoch 23 loss 0.14554187655448914\n",
      "Epoch 24 loss 0.13750937581062317\n",
      "Epoch 25 loss 0.12993453443050385\n",
      "Epoch 26 loss 0.12278727442026138\n",
      "Epoch 27 loss 0.11604902893304825\n",
      "Epoch 28 loss 0.10969187319278717\n",
      "Epoch 29 loss 0.10368959605693817\n",
      "Epoch 30 loss 0.09802678227424622\n",
      "Epoch 31 loss 0.09268353879451752\n",
      "Epoch 32 loss 0.08763682842254639\n",
      "Epoch 33 loss 0.08287206292152405\n",
      "Epoch 34 loss 0.07837390154600143\n",
      "Epoch 35 loss 0.07412444055080414\n",
      "Epoch 36 loss 0.07011201232671738\n",
      "Epoch 37 loss 0.06632505357265472\n",
      "Epoch 38 loss 0.06274470686912537\n",
      "Epoch 39 loss 0.05936074256896973\n",
      "Epoch 40 loss 0.056168295443058014\n",
      "Epoch 41 loss 0.0531463697552681\n",
      "Epoch 42 loss 0.05029481649398804\n",
      "Epoch 43 loss 0.047597743570804596\n",
      "Epoch 44 loss 0.045048803091049194\n",
      "Epoch 45 loss 0.04263922944664955\n",
      "Epoch 46 loss 0.04036077857017517\n",
      "Epoch 47 loss 0.038207657635211945\n",
      "Epoch 48 loss 0.036169521510601044\n",
      "Epoch 49 loss 0.03424308821558952\n",
      "Epoch 50 loss 0.03242134302854538\n",
      "Epoch 51 loss 0.03070048987865448\n",
      "Epoch 52 loss 0.029069989919662476\n",
      "Epoch 53 loss 0.02752947434782982\n",
      "Epoch 54 loss 0.02607128955423832\n",
      "Epoch 55 loss 0.02469184622168541\n",
      "Epoch 56 loss 0.023387376219034195\n",
      "Epoch 57 loss 0.022150376811623573\n",
      "Epoch 58 loss 0.0209824126213789\n",
      "Epoch 59 loss 0.0198763869702816\n",
      "Epoch 60 loss 0.018829766660928726\n",
      "Epoch 61 loss 0.017839033156633377\n",
      "Epoch 62 loss 0.01690155640244484\n",
      "Epoch 63 loss 0.01601293683052063\n",
      "Epoch 64 loss 0.015172052197158337\n",
      "Epoch 65 loss 0.014377720654010773\n",
      "Epoch 66 loss 0.013624976389110088\n",
      "Epoch 67 loss 0.012911796569824219\n",
      "Epoch 68 loss 0.012236915528774261\n",
      "Epoch 69 loss 0.011598074808716774\n",
      "Epoch 70 loss 0.010992544703185558\n",
      "Epoch 71 loss 0.010419533587992191\n",
      "Epoch 72 loss 0.009877916425466537\n",
      "Epoch 73 loss 0.009363303892314434\n",
      "Epoch 74 loss 0.008876550942659378\n",
      "Epoch 75 loss 0.008414173498749733\n",
      "Epoch 76 loss 0.007977904751896858\n",
      "Epoch 77 loss 0.0075639234855771065\n",
      "Epoch 78 loss 0.007171609438955784\n",
      "Epoch 79 loss 0.006800293922424316\n",
      "Epoch 80 loss 0.006447927560657263\n",
      "Epoch 81 loss 0.006114637944847345\n",
      "Epoch 82 loss 0.00579808047041297\n",
      "Epoch 83 loss 0.005498350132256746\n",
      "Epoch 84 loss 0.005213866475969553\n",
      "Epoch 85 loss 0.004945044405758381\n",
      "Epoch 86 loss 0.00468964409083128\n",
      "Epoch 87 loss 0.004447999410331249\n",
      "Epoch 88 loss 0.004218935966491699\n",
      "Epoch 89 loss 0.004002218134701252\n",
      "Epoch 90 loss 0.003796330885961652\n",
      "Epoch 91 loss 0.003601283533498645\n",
      "Epoch 92 loss 0.0034153207670897245\n",
      "Epoch 93 loss 0.003240395337343216\n",
      "Epoch 94 loss 0.003073525382205844\n",
      "Epoch 95 loss 0.0029155502561479807\n",
      "Epoch 96 loss 0.0027666338719427586\n",
      "Epoch 97 loss 0.002624618588015437\n",
      "Epoch 98 loss 0.0024901549331843853\n",
      "Epoch 99 loss 0.002362748607993126\n"
     ]
    }
   ],
   "source": [
    "##specify the number of epochs\n",
    "# *****START CODE\n",
    "epochs = 100\n",
    "# *****END CODE\n",
    "\n",
    "for t in range(epochs):\n",
    "  # *****START CODE\n",
    "\n",
    "  # forward pass\n",
    "  h = x @ w1\n",
    "  h_relu = F.relu(h)\n",
    "  y_pred = h_relu @ w2\n",
    "  \n",
    "  # compute the loss\n",
    "  loss = ((y_pred - y) ** 2).mean()\n",
    "\n",
    "  # compute the gradient wrt w2\n",
    "  d_loss_d_y_pred = 2.0 * (y_pred - y)\n",
    "  d_y_pred_d_w2 = h_relu\n",
    "  d_loss_d_w2 = d_y_pred_d_w2.T @ d_loss_d_y_pred\n",
    "\n",
    "  # compute the gradient wrt w1\n",
    "  d_loss_d_y_pred = 2.0 * (y_pred - y)\n",
    "  d_y_pred_d_h = w2\n",
    "  d_h_d_w1 = x\n",
    "  d_loss_d_w1 = d_loss_d_y_pred @ d_y_pred_d_h.t()\n",
    "  d_loss_d_w1 = d_h_d_w1.t() @ d_loss_d_w1\n",
    "\n",
    "  # update the weights\n",
    "  w1 = w1 - learning_rate * d_loss_d_w1\n",
    "  w2 = w2 - learning_rate * d_loss_d_w2\n",
    "\n",
    "  print('Epoch', t, 'loss', loss.item())\n",
    "\n",
    "  # *****END CODE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4395, -0.7543, -1.1628,  2.0784, -0.1075, -0.6689,  2.8065,  0.2962,\n",
      "        -1.5878, -0.8216])\n"
     ]
    }
   ],
   "source": [
    "print(y[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4243, -0.7883, -1.1590,  2.0786, -0.1146, -0.6894,  2.8130,  0.3624,\n",
      "        -1.6275, -0.8242])\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
